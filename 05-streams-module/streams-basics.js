// Streams Module - Basic Examples
// ================================

const fs = require('fs');
const path = require('path');

console.log('=== Streams Module à¦¶à¦¿à¦–à¦¿ ===\n');

// à§§. Readable Stream - File à¦ªà¦¡à¦¼à¦¾ (Chunk by Chunk)
// -------------------------------------------------
console.log('à§§. Readable Stream - Reading File:');
console.log('â”€'.repeat(50));

const readStream = fs.createReadStream(
  path.join(__dirname, 'test-data', 'small-file.txt'),
  { encoding: 'utf8' }
);

console.log('Reading file in chunks...\n');

let chunkCount = 0;

readStream.on('data', (chunk) => {
  chunkCount++;
  console.log(`Chunk ${chunkCount}:`);
  console.log(chunk);
  console.log('â”€'.repeat(30));
});

readStream.on('end', () => {
  console.log(`\nâœ… File read complete! Total chunks: ${chunkCount}\n`);
});

readStream.on('error', (err) => {
  console.error('Error reading file:', err.message);
});

// à§¨. Writable Stream - File à¦²à¦¿à¦–à¦¾
// --------------------------------
setTimeout(() => {
  console.log('à§¨. Writable Stream - Writing File:');
  console.log('â”€'.repeat(50));
  
  const writeStream = fs.createWriteStream(
    path.join(__dirname, 'output', 'output.txt')
  );
  
  console.log('Writing data to file...\n');
  
  writeStream.write('Line 1: Hello from Streams!\n');
  writeStream.write('Line 2: Writing data chunk by chunk\n');
  writeStream.write('Line 3: This is efficient!\n');
  
  // Write à¦¶à§‡à¦· à¦•à¦°à§‹
  writeStream.end('Line 4: Final line\n');
  
  writeStream.on('finish', () => {
    console.log('âœ… File write complete!\n');
  });
  
  writeStream.on('error', (err) => {
    console.error('Error writing file:', err.message);
  });
}, 500);

// à§©. Pipe - Readable à¦¥à§‡à¦•à§‡ Writable à¦ à¦¸à¦°à¦¾à¦¸à¦°à¦¿
// ------------------------------------------
setTimeout(() => {
  console.log('à§©. Pipe - Copy File using Streams:');
  console.log('â”€'.repeat(50));
  
  const source = fs.createReadStream(
    path.join(__dirname, 'test-data', 'small-file.txt')
  );
  
  const destination = fs.createWriteStream(
    path.join(__dirname, 'output', 'copied.txt')
  );
  
  console.log('Copying file using pipe...\n');
  
  // Pipe à¦•à¦°à§‹ - automatically data transfer à¦¹à¦¬à§‡
  source.pipe(destination);
  
  destination.on('finish', () => {
    console.log('âœ… File copied successfully using pipe!\n');
  });
  
  source.on('error', (err) => {
    console.error('Source error:', err.message);
  });
  
  destination.on('error', (err) => {
    console.error('Destination error:', err.message);
  });
}, 1000);

// à§ª. Memory Usage Comparison
// --------------------------
setTimeout(() => {
  console.log('à§ª. Memory Usage - Stream vs Normal:');
  console.log('â”€'.repeat(50));
  
  const largeFile = path.join(__dirname, 'test-data', 'large-file.txt');
  
  // Method 1: Normal (readFile) - à¦ªà§à¦°à§‹ file memory à¦¤à§‡
  console.log('Method 1: fs.readFile (Load everything in memory)');
  const startMem1 = process.memoryUsage().heapUsed;
  
  fs.readFile(largeFile, 'utf8', (err, data) => {
    if (err) {
      console.error('Error:', err.message);
      return;
    }
    
    const endMem1 = process.memoryUsage().heapUsed;
    const used1 = ((endMem1 - startMem1) / 1024 / 1024).toFixed(2);
    
    console.log(`  File size: ${(data.length / 1024 / 1024).toFixed(2)} MB`);
    console.log(`  Memory used: ~${used1} MB`);
    console.log(`  ðŸ’¡ à¦ªà§à¦°à§‹ file memory à¦¤à§‡ load à¦¹à¦¯à¦¼à§‡à¦›à§‡!\n`);
    
    // Method 2: Stream - à¦¶à§à¦§à§ chunks
    setTimeout(() => {
      console.log('Method 2: createReadStream (Process chunks)');
      const startMem2 = process.memoryUsage().heapUsed;
      
      const stream = fs.createReadStream(largeFile, {
        encoding: 'utf8',
        highWaterMark: 64 * 1024 // 64KB chunks
      });
      
      let dataSize = 0;
      
      stream.on('data', (chunk) => {
        dataSize += chunk.length;
      });
      
      stream.on('end', () => {
        const endMem2 = process.memoryUsage().heapUsed;
        const used2 = ((endMem2 - startMem2) / 1024 / 1024).toFixed(2);
        
        console.log(`  File size: ${(dataSize / 1024 / 1024).toFixed(2)} MB`);
        console.log(`  Memory used: ~${used2} MB`);
        console.log(`  ðŸ’¡ à¦¶à§à¦§à§ chunks memory à¦¤à§‡ à¦›à¦¿à¦²!\n`);
        
        console.log('ðŸŽ¯ Result: Streams use MUCH less memory!\n');
      });
    }, 100);
  });
}, 1500);

// à§«. Backpressure Handling
// ------------------------
setTimeout(() => {
  console.log('à§«. Backpressure - Write Speed Control:');
  console.log('â”€'.repeat(50));
  
  const readable = fs.createReadStream(
    path.join(__dirname, 'test-data', 'large-file.txt')
  );
  
  const writable = fs.createWriteStream(
    path.join(__dirname, 'output', 'backpressure-test.txt')
  );
  
  readable.on('data', (chunk) => {
    const canWrite = writable.write(chunk);
    
    if (!canWrite) {
      console.log('âš ï¸  Buffer full! Pausing read stream...');
      readable.pause();
    }
  });
  
  writable.on('drain', () => {
    console.log('âœ… Buffer drained! Resuming read stream...');
    readable.resume();
  });
  
  readable.on('end', () => {
    writable.end();
    console.log('âœ… Backpressure handling complete!\n');
  });
}, 3000);

// à§¬. Stream Options - highWaterMark
// ---------------------------------
setTimeout(() => {
  console.log('à§¬. Stream Options - Chunk Size Control:');
  console.log('â”€'.repeat(50));
  
  console.log('Small chunks (16 bytes):');
  const smallChunks = fs.createReadStream(
    path.join(__dirname, 'test-data', 'small-file.txt'),
    { encoding: 'utf8', highWaterMark: 16 }
  );
  
  let smallCount = 0;
  smallChunks.on('data', (chunk) => {
    smallCount++;
    console.log(`  Chunk ${smallCount}: ${chunk.length} bytes`);
  });
  
  smallChunks.on('end', () => {
    console.log(`Total chunks with 16 bytes: ${smallCount}\n`);
    
    // Large chunks
    console.log('Large chunks (1024 bytes):');
    const largeChunks = fs.createReadStream(
      path.join(__dirname, 'test-data', 'small-file.txt'),
      { encoding: 'utf8', highWaterMark: 1024 }
    );
    
    let largeCount = 0;
    largeChunks.on('data', (chunk) => {
      largeCount++;
      console.log(`  Chunk ${largeCount}: ${chunk.length} bytes`);
    });
    
    largeChunks.on('end', () => {
      console.log(`Total chunks with 1024 bytes: ${largeCount}\n`);
    });
  });
}, 4000);

// à§­. Reading Control - pause() and resume()
// ------------------------------------------
setTimeout(() => {
  console.log('à§­. Stream Control - pause() and resume():');
  console.log('â”€'.repeat(50));
  
  const controlStream = fs.createReadStream(
    path.join(__dirname, 'test-data', 'small-file.txt'),
    { encoding: 'utf8' }
  );
  
  let isPaused = false;
  
  controlStream.on('data', (chunk) => {
    console.log('ðŸ“– Reading data...');
    
    if (!isPaused) {
      console.log('â¸ï¸  Pausing for 1 second...\n');
      controlStream.pause();
      isPaused = true;
      
      setTimeout(() => {
        console.log('â–¶ï¸  Resuming...');
        controlStream.resume();
        isPaused = false;
      }, 1000);
    }
  });
  
  controlStream.on('end', () => {
    console.log('âœ… Stream control demo complete!\n');
  });
}, 5500);

// à§®. Stream Close Event
// ---------------------
setTimeout(() => {
  console.log('à§®. Stream Lifecycle Events:');
  console.log('â”€'.repeat(50));
  
  const lifecycleStream = fs.createReadStream(
    path.join(__dirname, 'test-data', 'small-file.txt')
  );
  
  lifecycleStream.on('open', (fd) => {
    console.log('ðŸ”“ Stream opened (file descriptor:', fd + ')');
  });
  
  lifecycleStream.on('ready', () => {
    console.log('âœ… Stream ready to read');
  });
  
  lifecycleStream.on('data', () => {
    console.log('ðŸ“¦ Data chunk received');
  });
  
  lifecycleStream.on('end', () => {
    console.log('ðŸ Stream ended (no more data)');
  });
  
  lifecycleStream.on('close', () => {
    console.log('ðŸ”’ Stream closed\n');
  });
}, 7000);

setTimeout(() => {
  console.log('=== Streams Basics Complete! ===');
}, 8500);